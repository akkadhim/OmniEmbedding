{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ab8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset words count:  48\n",
      "Dataset: rg-65, Number of pairs: 65, Number of words: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading token embeddings: 100%|██████████| 37/37 [00:00<00:00, 384131.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman TM: 0.471\n",
      "Cosine TM \n",
      "[[ 1.         -0.18389329]\n",
      " [-0.18389329  1.        ]]\n",
      "Pearson TM \n",
      "[[1.         0.45475507]\n",
      " [0.45475507 1.        ]]\n",
      "Kendal TM: 0.3503817574348747\n",
      "Pearson Corr \n",
      "           Original        TM\n",
      "Original  1.000000  0.454755\n",
      "TM        0.454755  1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from tools import Tools\n",
    "import tqdm\n",
    "\n",
    "target_similarity=defaultdict(list)\n",
    "\n",
    "def calculate(target_similarity, pair_list):\n",
    "    calculated_score=[]\n",
    "    extracted_list = []\n",
    "    original_score=[]\n",
    "    word_pairs=[]\n",
    "    \n",
    "    for (x,y) in pair_list:\n",
    "        if x in target_similarity:\n",
    "            word1, word2=x\n",
    "            word1_prof = target_similarity[x] \n",
    "            extracted_list.append((x, word1_prof))\n",
    "            calculated_score.append(word1_prof)\n",
    "            original_score.append(y)\n",
    "            word_pairs.append(x)\n",
    "\n",
    "    spearman_TM = spearmanr(original_score, calculated_score)\n",
    "    spearman_TM = round(spearman_TM[0], 3)\n",
    "    print(f'Spearman TM: {spearman_TM}')\n",
    "\n",
    "    total_list=[]\n",
    "    total_list.append(original_score)\n",
    "    total_list.append(calculated_score)\n",
    "\n",
    "    similarity = cosine_similarity(total_list)\n",
    "    print(f'Cosine TM \\n{similarity}')\n",
    "\n",
    "    TM_corr= np.corrcoef(original_score, calculated_score)\n",
    "    print(f'Pearson TM \\n{TM_corr}')\n",
    "\n",
    "    kendal_TM, _ = kendalltau(original_score, calculated_score)\n",
    "    print(f'Kendal TM: {kendal_TM}')\n",
    "\n",
    "    data = pd.DataFrame([original_score,calculated_score])\n",
    "    data=data.transpose()\n",
    "    data.columns=['Original','TM']\n",
    "    correlation = data.corr()\n",
    "    print(\"Pearson Corr \\n\", correlation)\n",
    "    return spearman_TM\n",
    "\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "omni_embeddings = Tools.read_pickle_data(\"omni_embeddings.pickle\")\n",
    "\n",
    "target_dataset_name = \"rg-65\"\n",
    "# target_dataset_name = \"wordsim353-sim\"\n",
    "# target_dataset_name = \"mturk-287\"\n",
    "# target_dataset_name = \"mturk-771\"\n",
    "# target_dataset_name = \"simlex999\"\n",
    "# target_dataset_name = \"men\"\n",
    "\n",
    "pair_list = Tools.get_dataset_pairs(target_dataset_name + \".csv\")\n",
    "output_active, target_words = Tools.get_dataset_targets(target_dataset_name, vectorizer_X, pair_list)\n",
    "print(f\"Dataset: {target_dataset_name}, Number of pairs: {len(pair_list)}, Number of words: {len(target_words)}\")\n",
    "token_embeddings = {}\n",
    "for word in tqdm.tqdm(target_words, desc=\"Loading token embeddings\"):\n",
    "    word_id = vectorizer_X.vocabulary_.get(word, None)\n",
    "    if word_id is not None:\n",
    "        token_embeddings[word_id] = omni_embeddings[word_id]\n",
    "\n",
    "profile = np.empty((len(target_words), number_of_features))\n",
    "for i, word in enumerate(target_words):\n",
    "    word_id = vectorizer_X.vocabulary_.get(word, None)\n",
    "    if word_id is not None and word_id in token_embeddings:\n",
    "        profile[i, :] = token_embeddings[word_id]\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found in vocabulary or embeddings.\")\n",
    "\n",
    "for i, word1 in enumerate(target_words):\n",
    "    for j, word2 in enumerate(target_words):\n",
    "        if i != j:\n",
    "            word2_index = vectorizer_X.vocabulary_.get(word2, None)\n",
    "            if word2_index is not None:\n",
    "                target_similarity[(word1, word2)] = profile[i, word2_index]\n",
    "            else:\n",
    "                target_similarity[(word1, word2)] = 0.0\n",
    "\n",
    "spearman = calculate(target_similarity,pair_list)\n",
    "token_embeddings = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
