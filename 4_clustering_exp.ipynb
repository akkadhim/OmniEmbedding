{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b32856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'airplane' not found in token embeddings.\n",
      "Word 'anger' not found in token embeddings.\n",
      "Word 'animal' not found in token embeddings.\n",
      "Word 'apple' not found in token embeddings.\n",
      "Word 'art' not found in token embeddings.\n",
      "Word 'artist' not found in token embeddings.\n",
      "Word 'baseball' not found in token embeddings.\n",
      "Word 'basketball' not found in token embeddings.\n",
      "Word 'beauty' not found in token embeddings.\n",
      "Word 'bicycle' not found in token embeddings.\n",
      "Word 'bird' not found in token embeddings.\n",
      "Word 'boat' not found in token embeddings.\n",
      "Word 'book' not found in token embeddings.\n",
      "Word 'boxing' not found in token embeddings.\n",
      "Word 'bread' not found in token embeddings.\n",
      "Word 'brother' not found in token embeddings.\n",
      "Word 'butterfly' not found in token embeddings.\n",
      "Word 'cake' not found in token embeddings.\n",
      "Word 'calm' not found in token embeddings.\n",
      "Word 'camera' not found in token embeddings.\n",
      "Word 'car' not found in token embeddings.\n",
      "Word 'cat' not found in token embeddings.\n",
      "Word 'cheese' not found in token embeddings.\n",
      "Word 'chef' not found in token embeddings.\n",
      "Word 'child' not found in token embeddings.\n",
      "Word 'chocolate' not found in token embeddings.\n",
      "Word 'city' not found in token embeddings.\n",
      "Word 'cloud' not found in token embeddings.\n",
      "Word 'coffee' not found in token embeddings.\n",
      "Word 'computer' not found in token embeddings.\n",
      "Word 'continent' not found in token embeddings.\n",
      "Word 'country' not found in token embeddings.\n",
      "Word 'cricket' not found in token embeddings.\n",
      "Word 'desert' not found in token embeddings.\n",
      "Word 'desert' not found in token embeddings.\n",
      "Word 'doctor' not found in token embeddings.\n",
      "Word 'dog' not found in token embeddings.\n",
      "Word 'education' not found in token embeddings.\n",
      "Word 'elephant' not found in token embeddings.\n",
      "Word 'engineer' not found in token embeddings.\n",
      "Word 'excited' not found in token embeddings.\n",
      "Word 'faith' not found in token embeddings.\n",
      "Word 'father' not found in token embeddings.\n",
      "Word 'fear' not found in token embeddings.\n",
      "Word 'fish' not found in token embeddings.\n",
      "Word 'flower' not found in token embeddings.\n",
      "Word 'forest' not found in token embeddings.\n",
      "Word 'freedom' not found in token embeddings.\n",
      "Word 'friend' not found in token embeddings.\n",
      "Word 'golf' not found in token embeddings.\n",
      "Word 'governor' not found in token embeddings.\n",
      "Word 'happy' not found in token embeddings.\n",
      "Word 'hardware' not found in token embeddings.\n",
      "Word 'hate' not found in token embeddings.\n",
      "Word 'helicopter' not found in token embeddings.\n",
      "Word 'history' not found in token embeddings.\n",
      "Word 'hockey' not found in token embeddings.\n",
      "Word 'husband' not found in token embeddings.\n",
      "Word 'internet' not found in token embeddings.\n",
      "Word 'island' not found in token embeddings.\n",
      "Word 'joy' not found in token embeddings.\n",
      "Word 'justice' not found in token embeddings.\n",
      "Word 'king' not found in token embeddings.\n",
      "Word 'knowledge' not found in token embeddings.\n",
      "Word 'lake' not found in token embeddings.\n",
      "Word 'language' not found in token embeddings.\n",
      "Word 'lawyer' not found in token embeddings.\n",
      "Word 'leader' not found in token embeddings.\n",
      "Word 'lion' not found in token embeddings.\n",
      "Word 'love' not found in token embeddings.\n",
      "Word 'mayor' not found in token embeddings.\n",
      "Word 'medicine' not found in token embeddings.\n",
      "Word 'milk' not found in token embeddings.\n",
      "Word 'minister' not found in token embeddings.\n",
      "Word 'mother' not found in token embeddings.\n",
      "Word 'motorcycle' not found in token embeddings.\n",
      "Word 'mountain' not found in token embeddings.\n",
      "Word 'mountain' not found in token embeddings.\n",
      "Word 'movie' not found in token embeddings.\n",
      "Word 'music' not found in token embeddings.\n",
      "Word 'neighbor' not found in token embeddings.\n",
      "Word 'nervous' not found in token embeddings.\n",
      "Word 'network' not found in token embeddings.\n",
      "Word 'nurse' not found in token embeddings.\n",
      "Word 'ocean' not found in token embeddings.\n",
      "Word 'partner' not found in token embeddings.\n",
      "Word 'peace' not found in token embeddings.\n",
      "Word 'philosophy' not found in token embeddings.\n",
      "Word 'phone' not found in token embeddings.\n",
      "Word 'pilot' not found in token embeddings.\n",
      "Word 'pizza' not found in token embeddings.\n",
      "Word 'power' not found in token embeddings.\n",
      "Word 'president' not found in token embeddings.\n",
      "Word 'prince' not found in token embeddings.\n",
      "Word 'princess' not found in token embeddings.\n",
      "Word 'queen' not found in token embeddings.\n",
      "Word 'rice' not found in token embeddings.\n",
      "Word 'river' not found in token embeddings.\n",
      "Word 'river' not found in token embeddings.\n",
      "Word 'robot' not found in token embeddings.\n",
      "Word 'running' not found in token embeddings.\n",
      "Word 'sad' not found in token embeddings.\n",
      "Word 'science' not found in token embeddings.\n",
      "Word 'scientist' not found in token embeddings.\n",
      "Word 'senator' not found in token embeddings.\n",
      "Word 'ship' not found in token embeddings.\n",
      "Word 'sister' not found in token embeddings.\n",
      "Word 'sky' not found in token embeddings.\n",
      "Word 'soccer' not found in token embeddings.\n",
      "Word 'software' not found in token embeddings.\n",
      "Word 'soup' not found in token embeddings.\n",
      "Word 'subway' not found in token embeddings.\n",
      "Word 'swimming' not found in token embeddings.\n",
      "Word 'teacher' not found in token embeddings.\n",
      "Word 'tennis' not found in token embeddings.\n",
      "Word 'tiger' not found in token embeddings.\n",
      "Word 'train' not found in token embeddings.\n",
      "Word 'tree' not found in token embeddings.\n",
      "Word 'truck' not found in token embeddings.\n",
      "Word 'truth' not found in token embeddings.\n",
      "Word 'valley' not found in token embeddings.\n",
      "Word 'village' not found in token embeddings.\n",
      "Word 'war' not found in token embeddings.\n",
      "Word 'wealth' not found in token embeddings.\n",
      "Word 'whale' not found in token embeddings.\n",
      "Word 'wife' not found in token embeddings.\n",
      "Word 'writer' not found in token embeddings.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TSNE.__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m valid_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m valid_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m omni_embeddings]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Apply t-SNE on the subset_embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# tsne = TSNE(n_components=2, random_state=42, perplexity=5) \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m tsne \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m reduced_subset \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(subset_embeddings)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TSNE.__init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import Tools\n",
    "\n",
    "\n",
    "X_train = Tools.read_pickle_data(\"X.pickle\")\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "omni_embeddings = Tools.read_pickle_data(\"omni_embeddings.pickle\")\n",
    "\n",
    "words = sorted([\n",
    "    \"happy\", \"sad\", \"joy\", \"anger\", \"fear\", \"love\", \"hate\", \"excited\", \"nervous\", \"calm\",\n",
    "    # Professions\n",
    "    \"doctor\", \"engineer\", \"teacher\", \"lawyer\", \"artist\", \"scientist\", \"nurse\", \"chef\", \"pilot\", \"writer\",\n",
    "    # Nature\n",
    "    \"tree\", \"river\", \"mountain\", \"ocean\", \"flower\", \"desert\", \"forest\", \"sky\", \"cloud\", \"animal\",\n",
    "    # Technology\n",
    "    \"computer\", \"internet\", \"robot\", \"AI\", \"software\", \"hardware\", \"phone\", \"camera\", \"network\", \"algorithm\",\n",
    "    # Relationships\n",
    "    \"father\", \"mother\", \"brother\", \"sister\", \"friend\", \"husband\", \"wife\", \"child\", \"partner\", \"neighbor\",\n",
    "    # Food\n",
    "    \"bread\", \"apple\", \"pizza\", \"coffee\", \"chocolate\", \"milk\", \"soup\", \"rice\", \"cake\", \"cheese\",\n",
    "    # Geography\n",
    "    \"city\", \"village\", \"country\", \"continent\", \"river\", \"lake\", \"mountain\", \"valley\", \"desert\", \"island\",\n",
    "    # Abstract Concepts\n",
    "    \"freedom\", \"justice\", \"peace\", \"war\", \"knowledge\", \"power\", \"truth\", \"beauty\", \"faith\", \"wealth\",\n",
    "    # Animals\n",
    "    \"cat\", \"dog\", \"lion\", \"tiger\", \"elephant\", \"bird\", \"fish\", \"whale\", \"dolphin\", \"butterfly\",\n",
    "    # Vehicles\n",
    "    \"car\", \"truck\", \"bicycle\", \"train\", \"airplane\", \"ship\", \"boat\", \"motorcycle\", \"subway\", \"helicopter\",\n",
    "    # Sports\n",
    "    \"soccer\", \"basketball\", \"tennis\", \"cricket\", \"baseball\", \"golf\", \"hockey\", \"boxing\", \"running\", \"swimming\",\n",
    "    # Royalty/Leadership\n",
    "    \"king\", \"queen\", \"prince\", \"princess\", \"leader\", \"president\", \"minister\", \"senator\", \"governor\", \"mayor\",\n",
    "    # Miscellaneous\n",
    "    \"book\", \"music\", \"movie\", \"art\", \"language\", \"history\", \"science\", \"medicine\", \"education\", \"philosophy\"\n",
    "])\n",
    "\n",
    "\n",
    "# Define a subset of words that are valid (present in the vocabulary)\n",
    "valid_words = [word for word in words if word in vectorizer_X.vocabulary_]\n",
    "for word in valid_words:\n",
    "    if word not in omni_embeddings:\n",
    "        print(f\"Word '{word}' not found in token embeddings.\")\n",
    "        continue\n",
    "\n",
    "# Extract embeddings for valid words from token_embeddings\n",
    "subset_embeddings = np.array([omni_embeddings[word] for word in valid_words if word in omni_embeddings])\n",
    "\n",
    "# Ensure the length of valid_words matches the subset_embeddings\n",
    "valid_words = [word for word in valid_words if word in omni_embeddings]\n",
    "\n",
    "# Apply t-SNE on the subset_embeddings\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=5) \n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=2000, random_state=42)\n",
    "reduced_subset = tsne.fit_transform(subset_embeddings)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 6), dpi=100)  # Adjust figure size and resolution\n",
    "plt.scatter(reduced_subset[:, 0], reduced_subset[:, 1], alpha=0.7, color='#1f77b4', s=20)  # Reduced point size\n",
    "\n",
    "# Annotate the points with their corresponding words\n",
    "for i, word in enumerate(valid_words):\n",
    "    plt.text(reduced_subset[i, 0], reduced_subset[i, 1], word, fontsize=6)  # Reduced font size\n",
    "\n",
    "# Remove axis legends\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Save the plot as a PDF\n",
    "plt.savefig('word_embeddings_visualization.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot (optional, since we're saving it)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
