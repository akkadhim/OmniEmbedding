{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Words Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted([\n",
    "    \"happy\", \"sad\", \"joy\", \"anger\", \"fear\", \"love\", \"hate\", \"excited\", \"nervous\", \"calm\",\n",
    "    # Professions\n",
    "    \"doctor\", \"engineer\", \"teacher\", \"lawyer\", \"artist\", \"scientist\", \"nurse\", \"chef\", \"pilot\", \"writer\",\n",
    "    # Nature\n",
    "    \"tree\", \"river\", \"mountain\", \"ocean\", \"flower\", \"desert\", \"forest\", \"sky\", \"cloud\", \"animal\",\n",
    "    # Technology\n",
    "    \"computer\", \"internet\", \"robot\", \"AI\", \"software\", \"hardware\", \"phone\", \"camera\", \"network\", \"algorithm\",\n",
    "    # Relationships\n",
    "    \"father\", \"mother\", \"brother\", \"sister\", \"friend\", \"husband\", \"wife\", \"child\", \"partner\", \"neighbor\",\n",
    "    # Food\n",
    "    \"bread\", \"apple\", \"pizza\", \"coffee\", \"chocolate\", \"milk\", \"soup\", \"rice\", \"cake\", \"cheese\",\n",
    "    # Geography\n",
    "    \"city\", \"village\", \"country\", \"continent\", \"river\", \"lake\", \"mountain\", \"valley\", \"desert\", \"island\",\n",
    "    # Abstract Concepts\n",
    "    \"freedom\", \"justice\", \"peace\", \"war\", \"knowledge\", \"power\", \"truth\", \"beauty\", \"faith\", \"wealth\",\n",
    "    # Animals\n",
    "    \"cat\", \"dog\", \"lion\", \"tiger\", \"elephant\", \"bird\", \"fish\", \"whale\", \"dolphin\", \"butterfly\",\n",
    "    # Vehicles\n",
    "    \"car\", \"truck\", \"bicycle\", \"train\", \"airplane\", \"ship\", \"boat\", \"motorcycle\", \"subway\", \"helicopter\",\n",
    "    # Sports\n",
    "    \"soccer\", \"basketball\", \"tennis\", \"cricket\", \"baseball\", \"golf\", \"hockey\", \"boxing\", \"running\", \"swimming\",\n",
    "    # Royalty/Leadership\n",
    "    \"king\", \"queen\", \"prince\", \"princess\", \"leader\", \"president\", \"minister\", \"senator\", \"governor\", \"mayor\",\n",
    "    # Miscellaneous\n",
    "    \"book\", \"music\", \"movie\", \"art\", \"language\", \"history\", \"science\", \"medicine\", \"education\", \"philosophy\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity dataset folder: /workspace/mnt/data/datasets/similarity_datasets\n",
      "Total words: 48\n"
     ]
    }
   ],
   "source": [
    "import tools as Tools\n",
    "\n",
    "# dataset = 'mturk-771.csv'\n",
    "# dataset = 'mturk-287.csv'\n",
    "# dataset = 'wordsim353-sim.csv'\n",
    "dataset = 'rg-65.csv'\n",
    "words = []\n",
    "dataset_words = Tools.get_dataset_words(dataset)\n",
    "for word in dataset_words:\n",
    "    words.append(word)\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 40000\n",
      "First 10 words: ['aa' 'aaa' 'aaib' 'aalborg' 'aamer' 'aaplo' 'aar' 'aaron' 'aarp' 'ab']\n"
     ]
    }
   ],
   "source": [
    "from tools import Tools\n",
    "\n",
    "words = []\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "words = vectorizer_X.get_feature_names_out()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"First 10 words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omni TM-AE Collecting of embedding vectors for target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output folder: /workspace/mnt/data/knowledge_files/tm_1billion_40k_literals_2000_24_800_3200_5.0_25_20250425220152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training TM: 100%|██████████| 44/44 [3:51:59<00:00, 316.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tmu.models.autoencoder.autoencoder import TMAutoEncoder\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import tools as Tools\n",
    "\n",
    "\n",
    "X_train = Tools.read_pickle_data(\"X.pickle\")\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "\n",
    "# 1 Billion Parameters\n",
    "clause_weight_threshold = 0\n",
    "number_of_examples = 2000\n",
    "accumulation = 24\n",
    "clauses = 32\n",
    "T = 20000\n",
    "s = 1.0\n",
    "epochs = 4\n",
    "number_of_state_bits_ta = 8\n",
    "all_features = True\n",
    "\n",
    "# IMDB Parameters\n",
    "# clause_weight_threshold = 0\n",
    "# number_of_examples = 2000\n",
    "# accumulation = 24\n",
    "# clauses = 4\n",
    "# T = 3200\n",
    "# s = 1.0\n",
    "# epochs = 10\n",
    "# number_of_state_bits_ta = 8\n",
    "# all_features = True\n",
    "\n",
    "valid_words = []\n",
    "for word in words:\n",
    "    if word in vectorizer_X.vocabulary_:\n",
    "        word_id = vectorizer_X.vocabulary_[word]\n",
    "        valid_words.append((word, word_id))\n",
    "\n",
    "# Function to collect Omni embedding for a single word\n",
    "def train_word(word_data):\n",
    "    word, word_id = word_data\n",
    "    single_output_active = np.empty(1, dtype=np.uint32)\n",
    "    single_output_active[0] = word_id\n",
    "\n",
    "    tm = TMAutoEncoder(\n",
    "        number_of_clauses=clauses,\n",
    "        T=T,\n",
    "        s=s,\n",
    "        output_active=single_output_active,\n",
    "        max_included_literals=3,\n",
    "        accumulation=accumulation,\n",
    "        feature_negation=True,\n",
    "        platform='CPU', \n",
    "        output_balancing=0.5\n",
    "    )\n",
    "\n",
    "    for e in range(epochs):\n",
    "        tm.fit(X_train, number_of_examples=number_of_examples)\n",
    "    clauses_weights = tm.get_weights(0)\n",
    "\n",
    "    literal_sums = np.zeros(number_of_features)\n",
    "    literal_counts = np.zeros(number_of_features)\n",
    "    \n",
    "    for j in range(clauses):\n",
    "        clause_weight = clauses_weights[j]\n",
    "        if clause_weight > 0:\n",
    "            for i in range(tm.clause_bank.number_of_literals):\n",
    "                if i < number_of_features:\n",
    "                    literal_sums[i] += tm.get_ta_state(j, i)\n",
    "                    literal_counts[i] += 1\n",
    "                else:\n",
    "                    literal_sums[i - number_of_features] -= tm.get_ta_state(j, i)\n",
    "                    literal_counts[i - number_of_features] += 1\n",
    "\n",
    "    non_zero_counts = literal_counts > 0\n",
    "    embedding = np.zeros(number_of_features)\n",
    "    embedding[non_zero_counts] = (literal_sums[non_zero_counts] / literal_counts[non_zero_counts]).astype(int)\n",
    "    return embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #save for each word so can called it like  omni_embeddings.get(id, None)\n",
    "    all_embeddings = {}\n",
    "    for word_data in tqdm.tqdm(valid_words, desc=\"Training words\", unit=\"word\"):\n",
    "        embedding = train_word(word_data)\n",
    "        all_embeddings[word_data[1]] = embedding\n",
    "\n",
    "# save embeddings as pickle\n",
    "Tools.save_pickle_data(all_embeddings, \"omni_embeddings.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools as Tools\n",
    "X_train = Tools.read_pickle_data(\"X.pickle\")\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "feature_names = vectorizer_X.get_feature_names_out() \n",
    "    \n",
    "# Vocabulary dictionary: word -> column index\n",
    "vocabulary = vectorizer_X.vocabulary_\n",
    "# Reverse vocabulary: column index -> word\n",
    "reverse_vocab = {index: word for word, index in vocabulary.items()}\n",
    "# Reconstruct tokenized sentences from X_train\n",
    "tokenized_sentences = []\n",
    "\n",
    "for row in X_train:\n",
    "    # Extract indices of words that appear in this sentence\n",
    "    word_indices = row.indices  # Non-zero indices in the sparse matrix row\n",
    "    # Map indices to words using reverse vocabulary\n",
    "    sentence = [reverse_vocab[index] for index in word_indices]\n",
    "    tokenized_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vector_size = 100\n",
    "window = 5\n",
    "epochs = 25\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=vector_size,  # Dimensionality of embeddings\n",
    "    window=window,  # Context window size\n",
    "    min_count=1,  # Minimum word count to include in vocabulary\n",
    "    workers=4,  # Number of parallel threads\n",
    "    sg=1,  # Skip-gram model\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Save the trained Word2Vec model\n",
    "word2vec_file_name = \"custom_word2vec\"\n",
    "word2vec_model.save(word2vec_file_name + \".model\")\n",
    "# Save in binary format (compatible with KeyedVectors)\n",
    "word2vec_model.wv.save_word2vec_format(word2vec_file_name + \".bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last train time 166m and 42s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import os\n",
    "\n",
    "vector_size = 100  \n",
    "window = 5\n",
    "epochs = 25\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=vector_size,  # Dimensionality of embeddings\n",
    "    window=window,  # Context window size\n",
    "    min_count=1,  # Minimum word count to include in vocabulary\n",
    "    workers=4,  # Number of parallel threads\n",
    "    sg=1,  # Skip-gram model\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "file_name = \"custom_fasttext\"\n",
    "fasttext_model.save(file_name + \".model\")\n",
    "# Save in binary format (compatible with KeyedVectors)\n",
    "fasttext_model.wv.save_word2vec_format(file_name + \".bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last train time 321m and 59s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text8-like file with sentences saved to custom_glove_text8_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "# Prepare the text8-like file with sentences on new lines\n",
    "output_file = \"custom_glove_text8_sentences.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in tokenized_sentences:\n",
    "        # Join words in the sentence with spaces and write to the file\n",
    "        f.write(\" \".join(sentence) + \"\\n\")\n",
    "\n",
    "print(f\"Text8-like file with sentences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [GloVe GitHub Repository](https://github.com/stanfordnlp/GloVe)\n",
    "```bash\n",
    "git clone https://github.com/stanfordnlp/glove\n",
    "# Move the corpus to glove folder\n",
    "mv custom_glove_text8_sentences.txt glove/text8\n",
    "# Navigate to the GloVe directory and compile the code\n",
    "cd glove && make\n",
    "# Change hyperparameters in demo.sh\n",
    "# VECTOR_SIZE=100\n",
    "# MAX_ITER=25\n",
    "# WINDOW_SIZE=5\n",
    "\n",
    "# Run the demo script\n",
    "./demo.sh\n",
    "```\n",
    "### Last train time 25m and 20s\n",
    "#### output is vectors.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
