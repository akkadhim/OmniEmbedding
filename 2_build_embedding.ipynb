{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Words Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted([\n",
    "    \"happy\", \"sad\", \"joy\", \"anger\", \"fear\", \"love\", \"hate\", \"excited\", \"nervous\", \"calm\",\n",
    "    # Professions\n",
    "    \"doctor\", \"engineer\", \"teacher\", \"lawyer\", \"artist\", \"scientist\", \"nurse\", \"chef\", \"pilot\", \"writer\",\n",
    "    # Nature\n",
    "    \"tree\", \"river\", \"mountain\", \"ocean\", \"flower\", \"desert\", \"forest\", \"sky\", \"cloud\", \"animal\",\n",
    "    # Technology\n",
    "    \"computer\", \"internet\", \"robot\", \"AI\", \"software\", \"hardware\", \"phone\", \"camera\", \"network\", \"algorithm\",\n",
    "    # Relationships\n",
    "    \"father\", \"mother\", \"brother\", \"sister\", \"friend\", \"husband\", \"wife\", \"child\", \"partner\", \"neighbor\",\n",
    "    # Food\n",
    "    \"bread\", \"apple\", \"pizza\", \"coffee\", \"chocolate\", \"milk\", \"soup\", \"rice\", \"cake\", \"cheese\",\n",
    "    # Geography\n",
    "    \"city\", \"village\", \"country\", \"continent\", \"river\", \"lake\", \"mountain\", \"valley\", \"desert\", \"island\",\n",
    "    # Abstract Concepts\n",
    "    \"freedom\", \"justice\", \"peace\", \"war\", \"knowledge\", \"power\", \"truth\", \"beauty\", \"faith\", \"wealth\",\n",
    "    # Animals\n",
    "    \"cat\", \"dog\", \"lion\", \"tiger\", \"elephant\", \"bird\", \"fish\", \"whale\", \"dolphin\", \"butterfly\",\n",
    "    # Vehicles\n",
    "    \"car\", \"truck\", \"bicycle\", \"train\", \"airplane\", \"ship\", \"boat\", \"motorcycle\", \"subway\", \"helicopter\",\n",
    "    # Sports\n",
    "    \"soccer\", \"basketball\", \"tennis\", \"cricket\", \"baseball\", \"golf\", \"hockey\", \"boxing\", \"running\", \"swimming\",\n",
    "    # Royalty/Leadership\n",
    "    \"king\", \"queen\", \"prince\", \"princess\", \"leader\", \"president\", \"minister\", \"senator\", \"governor\", \"mayor\",\n",
    "    # Miscellaneous\n",
    "    \"book\", \"music\", \"movie\", \"art\", \"language\", \"history\", \"science\", \"medicine\", \"education\", \"philosophy\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 48\n"
     ]
    }
   ],
   "source": [
    "from tools import Tools\n",
    "\n",
    "# dataset = 'mturk-771.csv'\n",
    "# dataset = 'mturk-287.csv'\n",
    "# dataset = 'wordsim353-sim.csv'\n",
    "dataset = 'rg-65.csv'\n",
    "words = []\n",
    "dataset_words = Tools.get_dataset_words(dataset)\n",
    "for word in dataset_words:\n",
    "    words.append(word)\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 18896\n",
      "First 10 words: ['00' '000' '007' '01' '02' '05' '06' '10' '100' '1000']\n"
     ]
    }
   ],
   "source": [
    "from tools import Tools\n",
    "\n",
    "words = []\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "words = vectorizer_X.get_feature_names_out()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"First 10 words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omni TM-AE Collecting of embedding vectors for target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training words: 100%|██████████| 37/37 [02:55<00:00,  4.74s/word]\n"
     ]
    }
   ],
   "source": [
    "from tmu.models.autoencoder.autoencoder import TMAutoEncoder\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from tools import Tools\n",
    "\n",
    "\n",
    "X_train = Tools.read_pickle_data(\"X.pickle\")\n",
    "vectorizer_X = Tools.read_pickle_data(\"vectorizer_X.pickle\")\n",
    "number_of_features = vectorizer_X.get_feature_names_out().shape[0]\n",
    "feature_names = vectorizer_X.get_feature_names_out()\n",
    "\n",
    "# 1 Billion Parameters\n",
    "# clause_weight_threshold = 0\n",
    "# number_of_examples = 2000\n",
    "# accumulation = 24\n",
    "# clauses = 32\n",
    "# T = 20000\n",
    "# s = 1.0\n",
    "# epochs = 4\n",
    "# number_of_state_bits_ta = 8\n",
    "# all_features = True\n",
    "\n",
    "# IMDB Parameters\n",
    "clause_weight_threshold = 0\n",
    "number_of_examples = 2000\n",
    "accumulation = 24\n",
    "clauses = 32\n",
    "T = 3200\n",
    "s = 1.0\n",
    "epochs = 10\n",
    "number_of_state_bits_ta = 8\n",
    "all_features = True\n",
    "\n",
    "valid_words = []\n",
    "for word in words:\n",
    "    if word in vectorizer_X.vocabulary_:\n",
    "        word_id = vectorizer_X.vocabulary_[word]\n",
    "        valid_words.append((word, word_id))\n",
    "\n",
    "# Function to collect Omni embedding for a single word\n",
    "def train_word(word_data):\n",
    "    word, word_id = word_data\n",
    "    single_output_active = np.empty(1, dtype=np.uint32)\n",
    "    single_output_active[0] = word_id\n",
    "\n",
    "    tm = TMAutoEncoder(\n",
    "        number_of_clauses=clauses,\n",
    "        T=T,\n",
    "        s=s,\n",
    "        output_active=single_output_active,\n",
    "        max_included_literals=3,\n",
    "        accumulation=accumulation,\n",
    "        feature_negation=True,\n",
    "        platform='CPU', \n",
    "        output_balancing=0.5\n",
    "    )\n",
    "\n",
    "    for e in range(epochs):\n",
    "        tm.fit(X_train, number_of_examples=number_of_examples)\n",
    "    clauses_weights = tm.get_weights(0)\n",
    "\n",
    "    literal_sums = np.zeros(number_of_features)\n",
    "    literal_counts = np.zeros(number_of_features)\n",
    "    \n",
    "    for j in range(clauses):\n",
    "        clause_weight = clauses_weights[j]\n",
    "        if clause_weight > 0:\n",
    "            for i in range(tm.clause_bank.number_of_literals):\n",
    "                if i < number_of_features:\n",
    "                    literal_sums[i] += tm.get_ta_state(j, i)\n",
    "                    literal_counts[i] += 1\n",
    "                else:\n",
    "                    literal_sums[i - number_of_features] -= tm.get_ta_state(j, i)\n",
    "                    literal_counts[i - number_of_features] += 1\n",
    "\n",
    "    non_zero_counts = literal_counts > 0\n",
    "    embedding = np.zeros(number_of_features)\n",
    "    embedding[non_zero_counts] = (literal_sums[non_zero_counts] / literal_counts[non_zero_counts]).astype(int)\n",
    "    return embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #save for each word so can called it like  omni_embeddings.get(id, None)\n",
    "    all_embeddings = {}\n",
    "    for word_data in tqdm.tqdm(valid_words, desc=\"Training words\", unit=\"word\"):\n",
    "        embedding = train_word(word_data)\n",
    "        all_embeddings[word_data[1]] = embedding\n",
    "\n",
    "# save embeddings as pickle\n",
    "Tools.save_pickle_data(all_embeddings, \"omni_embeddings.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
